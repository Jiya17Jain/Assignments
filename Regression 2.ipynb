{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb144c1a-b436-40c6-a6d5-3b12f040ce59",
   "metadata": {},
   "source": [
    "Ans1.\n",
    "\n",
    "R-Squared (RÂ² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression model (the goodness of fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa69be-631f-417d-a9d4-65aa3bf095e3",
   "metadata": {},
   "source": [
    "Ans2.\n",
    "\n",
    "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases when the new term improves the model more than would be expected by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d2d65-e8fb-4a32-bf8a-fc401d724ea9",
   "metadata": {},
   "source": [
    "Ans3.\n",
    "\n",
    "It is better to use Adjusted R-squared when there are multiple variables in the regression model. This would allow us to compare models with differing numbers of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec151aa2-8d29-4293-a45f-77dd989d0b93",
   "metadata": {},
   "source": [
    "Ans4.\n",
    "\n",
    "Mean Squared Error(MSE) and Root Mean Square Error penalizes the large prediction errors . However, RMSE is widely used than MSE to evaluate the performance of the regression model with other random models as it has the same units as the dependent variable ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9faa7c3-3e46-4a54-a07c-1fef215c6dc3",
   "metadata": {},
   "source": [
    "Ans5.\n",
    "\n",
    "Advantage OF RMSE-\n",
    "RMSE is most useful when large errors are particularly undesirable.\n",
    "\n",
    "advantage of mse-\n",
    " The MSE is great for ensuring that our trained model has no outlier predictions with huge errors\n",
    " \n",
    " \n",
    "Advantage of MAE-\n",
    "It gives equal weight to all errors and is less sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf95ab-bf11-485e-b8ec-2a678eaa4bab",
   "metadata": {},
   "source": [
    "Ans6.\n",
    "\n",
    "Lasso regression, commonly referred to as L1 regularization, is a method for stopping overfitting in linear regression models by including a penalty term in the cost function. In contrast to Ridge regression, it adds the total of the absolute values of the coefficients rather than the sum of the squared coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb1546-e7fd-4bcd-bce0-5d474c413358",
   "metadata": {},
   "source": [
    "Ans7.\n",
    "\n",
    "Regularization helps to prevent overfitting, and can lead to better performance on new, unseen data. An example of a regularization term is the L2 regularization term, which adds a penalty based on the sum of the squares of the model's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa191b53-a80a-434b-92fb-3967e4aa4416",
   "metadata": {},
   "source": [
    "Ans8.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
